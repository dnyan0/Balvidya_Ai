# ğŸ“š Balvidya â€“ Subject-based Q&A System
Balvidya is an AI-powered educational assistant that helps students (Classes 5thâ€“10th) ask subject-specific questions and receive concise, accurate answers. It uses a Retrieval-Augmented Generation (RAG) pipeline powered by Google Gemini models and integrates with Streamlit for an interactive chat interface.

## ğŸš€ Features
- **Class & Subject Selection** â€“ Supports multiple classes (5thâ€“10th) and their subjects (Mathematics, Science, English, Physics, Chemistry, Biology, etc.).
- **RAG Pipeline** â€“ Uses FAISS vector stores for retrieving subject-relevant content from .docx study materials.
- **Document Parsing** â€“ Reads and splits class-specific documents into chunks for embedding.
- **Embeddings & LLM** â€“ Uses Google Generative AI embeddings (`models/embedding-001`) and Gemini chat model (`models/gemini-1.5-flash`).
- **Streamlit Chat UI** â€“ User-friendly chat interface with session-based history per (class, subject).
- **Context-Aware Answers** â€“ Priority is given to study material context, with limited fallback knowledge (basic math, grammar, general science).
- **Knowledge Base Management** â€“ Auto-creates or loads FAISS vector stores on first use.

## ğŸ§© Project Workflow
1.  User selects class & subject from Streamlit sidebar.
2.  System loads corresponding `.docx` file from `documents/{class}/{subject}.docx`.
3.  `rag.py` processes the document:
    - Reads DOCX via `python-docx`
    - Splits into chunks (500 tokens, 50 overlap)
    - Creates embeddings using Google Generative AI (`models/embedding-001`)
    - Builds a FAISS vector store and saves it locally
4.  User asks a question in chat input.
5.  Retrieval pipeline fetches relevant chunks from FAISS.
6.  Gemini Chat model (`gemini-1.5-flash`) generates an answer based on retrieved context.
7.  Answer is displayed in the Streamlit chat interface.

## ğŸ“‚ Project Structure
```bash 
Balvidya/
â”œâ”€â”€ app.py              # Main Streamlit entry point
â”œâ”€â”€ rag.py              # RAG pipeline (embeddings, vector store, QA)
â”œâ”€â”€ ui.py               # Sidebar & chat interface
â”œâ”€â”€ .env                # API key storage (GOOGLE_API_KEY)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ documents/          # Study material (class/subject DOCX files)
â””â”€â”€ vectorstore/        # Auto-generated FAISS indexes
```
## âš™ï¸ Setup & Installation
1.  **Clone repository**
    ```bash
    git clone [https://github.com/your-username/Balvidya.git](https://github.com/your-username/Balvidya.git)
    cd Balvidya
    ```
2.  **Create virtual environment**
    ```bash
    python -m venv venv
    source venv/bin/activate   # Linux/Mac
    venv\Scripts\activate      # Windows
    ```
3.  **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Setup API key**
    Create a `.env` file in the project root:
    ```
    GOOGLE_API_KEY=your_google_gemini_api_key
    ```
5.  **Run application**
    ```bash
    streamlit run app.py
    ```

## ğŸ§° Tech Stack
- Python 3.10+
- Streamlit â€“ Web UI framework
- LangChain â€“ RAG orchestration
- FAISS â€“ Vector database for retrieval
- Google Generative AI â€“
    - Embeddings: `models/embedding-001`
    - Chat model: `models/gemini-1.5-flash`
- `python-docx` â€“ DOCX parsing

## ğŸ¯ Current Capabilities
- Supports subject-based Q&A for 5thâ€“10th classes.
- Can only answer using provided class documents or limited fallback (basic math, grammar, general science).
- Maintains separate chat history per class & subject.
- Builds and caches FAISS vector stores for faster reuse.

## ğŸš¦ Limitations
- No multi-language support (currently English only).
- Requires .docx study material for each subject to answer effectively.
- Limited fallback knowledge â€“ cannot answer beyond allowed scope.
- Local-only storage (no deployment setup included yet).
